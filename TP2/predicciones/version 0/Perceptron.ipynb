{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def mi_split (df):\n",
    "\n",
    "    X = df.drop(['precio'], axis = 1)\n",
    "    Y = df['precio']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.35)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# x_train y_train son mi TRAIN\n",
    "# x_test y_test son mi VALIDACION\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def mean_target_encoding(train, nombrefeat, nombretarget):\n",
    "    \n",
    "    temp = train.groupby(nombrefeat)[nombretarget].transform(np.mean)\n",
    "    train[nombrefeat + \"_mean\"]=(temp-temp.min())/(temp.max()-temp.min())\n",
    "    \n",
    "   # temp = train.groupby(nombrefeat)[nombretarget].transform(np.mean) #Saco promedio\n",
    "   # temp = preprocessing.scale(temp) #Normalizo\n",
    "   # train[nombrefeat + \"_mean\"] = temp #Dejo en el DF\n",
    "    \n",
    "    return train\n",
    "\n",
    "def mean_target_decoding(x_test, nombrefeat, x_train):\n",
    "    \n",
    "    nombrefeatmean = nombrefeat + \"_mean\"\n",
    "    \n",
    "    temp = x_train.loc[:,[nombrefeat,nombrefeatmean]]\n",
    "    temp = temp.set_index(nombrefeat)\n",
    "    temp = temp.drop_duplicates()\n",
    "    temp = temp.T.squeeze()\n",
    "    values = x_test[nombrefeat].map(temp)\n",
    "    x_test[nombrefeatmean] = values \n",
    "\n",
    "    return x_test\n",
    "\n",
    "x_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\n",
    "def armar_set(train):\n",
    "    \n",
    "    #Puedo resolver de forma general las que son iguales para train y test\n",
    "    #es decir, aquellas que no filtran informacion a los de validacion\n",
    "    \n",
    "    train = train.drop(['descripcion', 'titulo', 'direccion', 'fecha', 'id'], axis = 1)\n",
    "    \n",
    "    train['servicios'] = train['piscina'] + train['gimnasio'] + train['usosmultiples']\n",
    "    \n",
    "    #Elimino los residuos\n",
    "    train = train.drop(['piscina', 'gimnasio','usosmultiples','escuelascercanas', \n",
    "                        'centroscomercialescercanos'], axis = 1)\n",
    "    \n",
    "    #Hago el split                    \n",
    "    x_train, x_test, y_train, y_test = mi_split(train)\n",
    "        \n",
    "    x_train[\"precio\"] = y_train\n",
    "\n",
    "    \n",
    "    #Calculo los mean target\n",
    "    x_train_mean = mean_target_encoding(x_train, \"provincia\", \"precio\")\n",
    "    x_train_mean['precio'] = y_train\n",
    "    x_train_mean = mean_target_encoding(x_train_mean, \"tipodepropiedad\", \"precio\")\n",
    "    x_train_mean = mean_target_encoding(x_train_mean, \"ciudad\", \"precio\")\n",
    "    \n",
    "    #Se los asigno a los test (NO LOS CALCULO CON ELLOS!!!!!!)\n",
    "    x_test = mean_target_decoding(x_test, \"provincia\", x_train_mean)\n",
    "    x_test = mean_target_decoding(x_test, \"tipodepropiedad\", x_train_mean)\n",
    "    x_test = mean_target_decoding(x_test, \"ciudad\", x_train_mean)\n",
    "\n",
    "    backup = x_train_mean\n",
    "    \n",
    "    x_train = x_train_mean.drop([\"precio\",\"provincia\",\"tipodepropiedad\",\"ciudad\"], axis=1)\n",
    "    x_test.drop([\"provincia\",\"tipodepropiedad\", \"ciudad\"], axis=1, inplace = True)\n",
    "\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, backup\n",
    "\n",
    "x_train, x_test, y_train, y_test, x_train_full = armar_set(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7094092572598320128.00000000\n",
      "Iteration 2, loss = 7128899699636708352.00000000\n",
      "Iteration 3, loss = 7123468206480669696.00000000\n",
      "Iteration 4, loss = 7118040850887426048.00000000\n",
      "Iteration 5, loss = 7112617630545178624.00000000\n",
      "Iteration 6, loss = 7107198541881694208.00000000\n",
      "Iteration 7, loss = 7101783582613612544.00000000\n",
      "Iteration 8, loss = 7096372748829576192.00000000\n",
      "Iteration 9, loss = 7090966037340255232.00000000\n",
      "Iteration 10, loss = 7085563445209064448.00000000\n",
      "Iteration 11, loss = 7080164969393682432.00000000\n",
      "Iteration 12, loss = 7074770606831702016.00000000\n",
      "Iteration 13, loss = 7069380354302403584.00000000\n",
      "Iteration 14, loss = 7063994207977942016.00000000\n",
      "Iteration 15, loss = 7058612165802839040.00000000\n",
      "Iteration 16, loss = 7053234224457266176.00000000\n",
      "Iteration 17, loss = 7047860379991738368.00000000\n",
      "Iteration 18, loss = 7042490630322119680.00000000\n",
      "Iteration 19, loss = 7037124971173235712.00000000\n",
      "Iteration 20, loss = 7031763400893176832.00000000\n",
      "Iteration 21, loss = 7026405915496541184.00000000\n",
      "Iteration 22, loss = 7021052511378658304.00000000\n",
      "Iteration 23, loss = 7015703186350982144.00000000\n",
      "Iteration 24, loss = 7010357937246016512.00000000\n",
      "Iteration 25, loss = 7005016760163970048.00000000\n",
      "Iteration 26, loss = 6999679652754257920.00000000\n",
      "Iteration 27, loss = 6994346611835224064.00000000\n",
      "Iteration 28, loss = 6989017634061084672.00000000\n",
      "Iteration 29, loss = 6983692716219665408.00000000\n",
      "Iteration 30, loss = 6978371855189878784.00000000\n",
      "Iteration 31, loss = 6973055048610300928.00000000\n",
      "Iteration 32, loss = 6967742292908788736.00000000\n",
      "Iteration 33, loss = 6962433584648889344.00000000\n",
      "Iteration 34, loss = 6957128921422410752.00000000\n",
      "Iteration 35, loss = 6951828299513512960.00000000\n",
      "Iteration 36, loss = 6946531716613634048.00000000\n",
      "Iteration 37, loss = 6941239168526559232.00000000\n",
      "Iteration 38, loss = 6935950653335926784.00000000\n",
      "Iteration 39, loss = 6930666167226232832.00000000\n",
      "Iteration 40, loss = 6925385706957786112.00000000\n",
      "Iteration 41, loss = 6920109270554514432.00000000\n",
      "Iteration 42, loss = 6914836854296623104.00000000\n",
      "Iteration 43, loss = 6909568454475880448.00000000\n",
      "Iteration 44, loss = 6904304068748762112.00000000\n",
      "Iteration 45, loss = 6899043694503290880.00000000\n",
      "Iteration 46, loss = 6893787327612961792.00000000\n",
      "Iteration 47, loss = 6888534965607031808.00000000\n",
      "Iteration 48, loss = 6883286605569750016.00000000\n",
      "Iteration 49, loss = 6878042243884546048.00000000\n",
      "Iteration 50, loss = 6872801878496355328.00000000\n",
      "Iteration 51, loss = 6867565505224137728.00000000\n",
      "Iteration 52, loss = 6862333121720145920.00000000\n",
      "Iteration 53, loss = 6857104724393963520.00000000\n",
      "Iteration 54, loss = 6851880310935132160.00000000\n",
      "Iteration 55, loss = 6846659877973330944.00000000\n",
      "Iteration 56, loss = 6841443422442365952.00000000\n",
      "Iteration 57, loss = 6836230941682795520.00000000\n",
      "Iteration 58, loss = 6831022431388151808.00000000\n",
      "Iteration 59, loss = 6825817890130341888.00000000\n",
      "Iteration 60, loss = 6820617313640608768.00000000\n",
      "Iteration 61, loss = 6815420699924207616.00000000\n",
      "Iteration 62, loss = 6810228045482048512.00000000\n",
      "Iteration 63, loss = 6805039346957350912.00000000\n",
      "Iteration 64, loss = 6799854602611022848.00000000\n",
      "Iteration 65, loss = 6794673807147315200.00000000\n",
      "Iteration 66, loss = 6789496959990772736.00000000\n",
      "Iteration 67, loss = 6784324056262146048.00000000\n",
      "Iteration 68, loss = 6779155094468311040.00000000\n",
      "Iteration 69, loss = 6773990070648650752.00000000\n",
      "Iteration 70, loss = 6768828981736157184.00000000\n",
      "Iteration 71, loss = 6763671825465478144.00000000\n",
      "Iteration 72, loss = 6758518598449456128.00000000\n",
      "Iteration 73, loss = 6753369297513672704.00000000\n",
      "Iteration 74, loss = 6748223919949625344.00000000\n",
      "Iteration 75, loss = 6743082462397197312.00000000\n",
      "Iteration 76, loss = 6737944922390896640.00000000\n",
      "Iteration 77, loss = 6732811296434179072.00000000\n",
      "Iteration 78, loss = 6727681581691228160.00000000\n",
      "Iteration 79, loss = 6722555775278458880.00000000\n",
      "Iteration 80, loss = 6717433874646989824.00000000\n",
      "Iteration 81, loss = 6712315876142365696.00000000\n",
      "Iteration 82, loss = 6707201776843735040.00000000\n",
      "Iteration 83, loss = 6702091573910303744.00000000\n",
      "Iteration 84, loss = 6696985264687494144.00000000\n",
      "Iteration 85, loss = 6691882846115821568.00000000\n",
      "Iteration 86, loss = 6686784314760536064.00000000\n",
      "Iteration 87, loss = 6681689668035678208.00000000\n",
      "Iteration 88, loss = 6676598903286291456.00000000\n",
      "Iteration 89, loss = 6671512016524019712.00000000\n",
      "Iteration 90, loss = 6666429005729288192.00000000\n",
      "Iteration 91, loss = 6661349867637898240.00000000\n",
      "Iteration 92, loss = 6656274599776375808.00000000\n",
      "Iteration 93, loss = 6651203198081882112.00000000\n",
      "Iteration 94, loss = 6646135660628210688.00000000\n",
      "Iteration 95, loss = 6641071983518317568.00000000\n",
      "Iteration 96, loss = 6636012165331152896.00000000\n",
      "Iteration 97, loss = 6630956201604895744.00000000\n",
      "Iteration 98, loss = 6625904090290333696.00000000\n",
      "Iteration 99, loss = 6620855828333825024.00000000\n",
      "Iteration 100, loss = 6615811412311487488.00000000\n",
      "Iteration 101, loss = 6610770839814339584.00000000\n",
      "Iteration 102, loss = 6605734107388432384.00000000\n",
      "Iteration 103, loss = 6600701212732749824.00000000\n",
      "Iteration 104, loss = 6595672152920431616.00000000\n",
      "Iteration 105, loss = 6590646924248605696.00000000\n",
      "Iteration 106, loss = 6585625524421785600.00000000\n",
      "Iteration 107, loss = 6580607950138432512.00000000\n",
      "Iteration 108, loss = 6575594199128143872.00000000\n",
      "Iteration 109, loss = 6570584267950929920.00000000\n",
      "Iteration 110, loss = 6565578154120329216.00000000\n",
      "Iteration 111, loss = 6560575853648277504.00000000\n",
      "Iteration 112, loss = 6555577364897006592.00000000\n",
      "Iteration 113, loss = 6550582684682142720.00000000\n",
      "Iteration 114, loss = 6545591809684499456.00000000\n",
      "Iteration 115, loss = 6540604737586576384.00000000\n",
      "Iteration 116, loss = 6535621465172838400.00000000\n",
      "Iteration 117, loss = 6530641988260037632.00000000\n",
      "Iteration 118, loss = 6525666306410612736.00000000\n",
      "Iteration 119, loss = 6520694415599998976.00000000\n",
      "Iteration 120, loss = 6515726311932461056.00000000\n",
      "Iteration 121, loss = 6510761994449830912.00000000\n",
      "Iteration 122, loss = 6505801458447856640.00000000\n",
      "Iteration 123, loss = 6500844701961785344.00000000\n",
      "Iteration 124, loss = 6495891722491843584.00000000\n",
      "Iteration 125, loss = 6490942516316248064.00000000\n",
      "Iteration 126, loss = 6485997081149566976.00000000\n",
      "Iteration 127, loss = 6481055413889225728.00000000\n",
      "Iteration 128, loss = 6476117511614996480.00000000\n",
      "Iteration 129, loss = 6471183372213388288.00000000\n",
      "Iteration 130, loss = 6466252991106454528.00000000\n",
      "Iteration 131, loss = 6461326367079659520.00000000\n",
      "Iteration 132, loss = 6456403495885678592.00000000\n",
      "Iteration 133, loss = 6451484375987502080.00000000\n",
      "Iteration 134, loss = 6446569003518269440.00000000\n",
      "Iteration 135, loss = 6441657376731684864.00000000\n",
      "Iteration 136, loss = 6436749491839764480.00000000\n",
      "Iteration 137, loss = 6431845346149735424.00000000\n",
      "Iteration 138, loss = 6426944936806059008.00000000\n",
      "Iteration 139, loss = 6422048261409225728.00000000\n",
      "Iteration 140, loss = 6417155316548337664.00000000\n",
      "Iteration 141, loss = 6412266099588336640.00000000\n",
      "Iteration 142, loss = 6407380608162613248.00000000\n",
      "Iteration 143, loss = 6402498838422692864.00000000\n",
      "Iteration 144, loss = 6397620788043368448.00000000\n",
      "Iteration 145, loss = 6392746454872692736.00000000\n",
      "Iteration 146, loss = 6387875834995803136.00000000\n",
      "Iteration 147, loss = 6383008925613949952.00000000\n",
      "Iteration 148, loss = 6378145724806089728.00000000\n",
      "Iteration 149, loss = 6373286229596897280.00000000\n",
      "Iteration 150, loss = 6368430436485583872.00000000\n",
      "Iteration 151, loss = 6363578342811020288.00000000\n",
      "Iteration 152, loss = 6358729945860362240.00000000\n",
      "Iteration 153, loss = 6353885243670880256.00000000\n",
      "Iteration 154, loss = 6349044232181743616.00000000\n",
      "Iteration 155, loss = 6344206908756072448.00000000\n",
      "Iteration 156, loss = 6339373271213483008.00000000\n",
      "Iteration 157, loss = 6334543316331397120.00000000\n",
      "Iteration 158, loss = 6329717041138441216.00000000\n",
      "Iteration 159, loss = 6324894443311663104.00000000\n",
      "Iteration 160, loss = 6320075519945400320.00000000\n",
      "Iteration 161, loss = 6315260268024697856.00000000\n",
      "Iteration 162, loss = 6310448685131499520.00000000\n",
      "Iteration 163, loss = 6305640767556403200.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 164, loss = 6300836513278106624.00000000\n",
      "Iteration 165, loss = 6296035919325250560.00000000\n",
      "Iteration 166, loss = 6291238983149077504.00000000\n",
      "Iteration 167, loss = 6286445702046186496.00000000\n",
      "Iteration 168, loss = 6281656072281480192.00000000\n",
      "Iteration 169, loss = 6276870092141698048.00000000\n",
      "Iteration 170, loss = 6272087757954905088.00000000\n",
      "Iteration 171, loss = 6267309068048697344.00000000\n",
      "Iteration 172, loss = 6262534018519147520.00000000\n",
      "Iteration 173, loss = 6257762607435634688.00000000\n",
      "Iteration 174, loss = 6252994831767545856.00000000\n",
      "Iteration 175, loss = 6248230688181753856.00000000\n",
      "Iteration 176, loss = 6243470174618780672.00000000\n",
      "Iteration 177, loss = 6238713288602425344.00000000\n",
      "Iteration 178, loss = 6233960026131241984.00000000\n",
      "Iteration 179, loss = 6229210385074777088.00000000\n",
      "Iteration 180, loss = 6224464363529167872.00000000\n",
      "Iteration 181, loss = 6219721957046579200.00000000\n",
      "Iteration 182, loss = 6214983164521675776.00000000\n",
      "Iteration 183, loss = 6210247982355349504.00000000\n",
      "Iteration 184, loss = 6205516407891248128.00000000\n",
      "Iteration 185, loss = 6200788437960039424.00000000\n",
      "Iteration 186, loss = 6196064070683609088.00000000\n",
      "Iteration 187, loss = 6191343302890374144.00000000\n",
      "Iteration 188, loss = 6186626132106768384.00000000\n",
      "Iteration 189, loss = 6181912554660482048.00000000\n",
      "Iteration 190, loss = 6177202568435964928.00000000\n",
      "Iteration 191, loss = 6172496171595579392.00000000\n",
      "Iteration 192, loss = 6167793359952747520.00000000\n",
      "Iteration 193, loss = 6163094131453126656.00000000\n",
      "Iteration 194, loss = 6158398483144322048.00000000\n",
      "Iteration 195, loss = 6153706412767268864.00000000\n",
      "Iteration 196, loss = 6149017916981228544.00000000\n",
      "Iteration 197, loss = 6144332993301569536.00000000\n",
      "Iteration 198, loss = 6139651639821407232.00000000\n",
      "Iteration 199, loss = 6134973851907431424.00000000\n",
      "Iteration 200, loss = 6130299628329324544.00000000\n",
      "Iteration 201, loss = 6125628966884062208.00000000\n",
      "Iteration 202, loss = 6120961863055805440.00000000\n",
      "Iteration 203, loss = 6116298315758706688.00000000\n",
      "Iteration 204, loss = 6111638320837150720.00000000\n",
      "Iteration 205, loss = 6106981877484634112.00000000\n",
      "Iteration 206, loss = 6102328980787992576.00000000\n",
      "Iteration 207, loss = 6097679629340548096.00000000\n",
      "Iteration 208, loss = 6093033820359203840.00000000\n",
      "Iteration 209, loss = 6088391551125230592.00000000\n",
      "Iteration 210, loss = 6083752819041460224.00000000\n",
      "Iteration 211, loss = 6079117620079640576.00000000\n",
      "Iteration 212, loss = 6074485953985540096.00000000\n",
      "Iteration 213, loss = 6069857816299145216.00000000\n",
      "Iteration 214, loss = 6065233204477058048.00000000\n",
      "Iteration 215, loss = 6060612116760645632.00000000\n",
      "Iteration 216, loss = 6055994549248871424.00000000\n",
      "Iteration 217, loss = 6051380500142228480.00000000\n",
      "Iteration 218, loss = 6046769966402889728.00000000\n",
      "Iteration 219, loss = 6042162945370978304.00000000\n",
      "Iteration 220, loss = 6037559434462621696.00000000\n",
      "Iteration 221, loss = 6032959430705861632.00000000\n",
      "Iteration 222, loss = 6028362931955113984.00000000\n",
      "Iteration 223, loss = 6023769935753149440.00000000\n",
      "Iteration 224, loss = 6019180438226192384.00000000\n",
      "Iteration 225, loss = 6014594437621168128.00000000\n",
      "Iteration 226, loss = 6010011931054392320.00000000\n",
      "Iteration 227, loss = 6005432915988024320.00000000\n",
      "Iteration 228, loss = 6000857389701339136.00000000\n",
      "Iteration 229, loss = 5996285349197267968.00000000\n",
      "Iteration 230, loss = 5991716792743196672.00000000\n",
      "Iteration 231, loss = 5987151716404887552.00000000\n",
      "Iteration 232, loss = 5982590118602628096.00000000\n",
      "Iteration 233, loss = 5978031996132849664.00000000\n",
      "Iteration 234, loss = 5973477346607857664.00000000\n",
      "Iteration 235, loss = 5968926166829512704.00000000\n",
      "Iteration 236, loss = 5964378455143090176.00000000\n",
      "Iteration 237, loss = 5959834207780344832.00000000\n",
      "Iteration 238, loss = 5955293423105502208.00000000\n",
      "Iteration 239, loss = 5950756098148048896.00000000\n",
      "Iteration 240, loss = 5946222229691626496.00000000\n",
      "Iteration 241, loss = 5941691816089591808.00000000\n",
      "Iteration 242, loss = 5937164853897415680.00000000\n",
      "Iteration 243, loss = 5932641340889356288.00000000\n",
      "Iteration 244, loss = 5928121274940124160.00000000\n",
      "Iteration 245, loss = 5923604651377693696.00000000\n",
      "Iteration 246, loss = 5919091469769465856.00000000\n",
      "Iteration 247, loss = 5914581727384613888.00000000\n",
      "Iteration 248, loss = 5910075419993761792.00000000\n",
      "Iteration 249, loss = 5905572546095945728.00000000\n",
      "Iteration 250, loss = 5901073103673181184.00000000\n",
      "Iteration 251, loss = 5896577088641585152.00000000\n",
      "Iteration 252, loss = 5892084499172019200.00000000\n",
      "Iteration 253, loss = 5887595332506276864.00000000\n",
      "Iteration 254, loss = 5883109586770532352.00000000\n",
      "Iteration 255, loss = 5878627258395369472.00000000\n",
      "Iteration 256, loss = 5874148344795749376.00000000\n",
      "Iteration 257, loss = 5869672844203141120.00000000\n",
      "Iteration 258, loss = 5865200753210143744.00000000\n",
      "Iteration 259, loss = 5860732069369632768.00000000\n",
      "Iteration 260, loss = 5856266790067507200.00000000\n",
      "Iteration 261, loss = 5851804913181092864.00000000\n",
      "Iteration 262, loss = 5847346436350858240.00000000\n",
      "Iteration 263, loss = 5842891355354169344.00000000\n",
      "Iteration 264, loss = 5838439669287585792.00000000\n",
      "Iteration 265, loss = 5833991374411689984.00000000\n",
      "Iteration 266, loss = 5829546468906987520.00000000\n",
      "Iteration 267, loss = 5825104950312353792.00000000\n",
      "Iteration 268, loss = 5820666815578855424.00000000\n",
      "Iteration 269, loss = 5816232061964311552.00000000\n",
      "Iteration 270, loss = 5811800687611107328.00000000\n",
      "Iteration 271, loss = 5807372689248621568.00000000\n",
      "Iteration 272, loss = 5802948064688517120.00000000\n",
      "Iteration 273, loss = 5798526810816632832.00000000\n",
      "Iteration 274, loss = 5794108925702364160.00000000\n",
      "Iteration 275, loss = 5789694407065511936.00000000\n",
      "Iteration 276, loss = 5785283251489436672.00000000\n",
      "Iteration 277, loss = 5780875456739864576.00000000\n",
      "Iteration 278, loss = 5776471019980824576.00000000\n",
      "Iteration 279, loss = 5772069939520612352.00000000\n",
      "Iteration 280, loss = 5767672211943903232.00000000\n",
      "Iteration 281, loss = 5763277834972385280.00000000\n",
      "Iteration 282, loss = 5758886806247234560.00000000\n",
      "Iteration 283, loss = 5754499122705494016.00000000\n",
      "Iteration 284, loss = 5750114782536525824.00000000\n",
      "Iteration 285, loss = 5745733782571410432.00000000\n",
      "Iteration 286, loss = 5741356120593655808.00000000\n",
      "Iteration 287, loss = 5736981793954835456.00000000\n",
      "Iteration 288, loss = 5732610799890576384.00000000\n",
      "Iteration 289, loss = 5728243135429715968.00000000\n",
      "Iteration 290, loss = 5723878800446149632.00000000\n",
      "Iteration 291, loss = 5719517789120243712.00000000\n",
      "Iteration 292, loss = 5715160100946487296.00000000\n",
      "Iteration 293, loss = 5710805733097820160.00000000\n",
      "Iteration 294, loss = 5706454682714576896.00000000\n",
      "Iteration 295, loss = 5702106947293339648.00000000\n",
      "Iteration 296, loss = 5697762524051089408.00000000\n",
      "Iteration 297, loss = 5693421411345810432.00000000\n",
      "Iteration 298, loss = 5689083606234435584.00000000\n",
      "Iteration 299, loss = 5684749106094133248.00000000\n",
      "Iteration 300, loss = 5680417907878937600.00000000\n",
      "Iteration 301, loss = 5676090009777572864.00000000\n",
      "Iteration 302, loss = 5671765409184343040.00000000\n",
      "Iteration 303, loss = 5667444103655032832.00000000\n",
      "Iteration 304, loss = 5663126090230928384.00000000\n",
      "Iteration 305, loss = 5658811366677629952.00000000\n",
      "Iteration 306, loss = 5654499930890648576.00000000\n",
      "Iteration 307, loss = 5650191779102052352.00000000\n",
      "Iteration 308, loss = 5645886910807881728.00000000\n",
      "Iteration 309, loss = 5641585321948417024.00000000\n",
      "Iteration 310, loss = 5637287010255923200.00000000\n",
      "Iteration 311, loss = 5632991973603560448.00000000\n",
      "Iteration 312, loss = 5628700209466055680.00000000\n",
      "Iteration 313, loss = 5624411714682961920.00000000\n",
      "Iteration 314, loss = 5620126487871045632.00000000\n",
      "Iteration 315, loss = 5615844525505992704.00000000\n",
      "Iteration 316, loss = 5611565826157855744.00000000\n",
      "Iteration 317, loss = 5607290386207139840.00000000\n",
      "Iteration 318, loss = 5603018204141229056.00000000\n",
      "Iteration 319, loss = 5598749276227473408.00000000\n",
      "Iteration 320, loss = 5594483601726953472.00000000\n",
      "Iteration 321, loss = 5590221176887934976.00000000\n",
      "Iteration 322, loss = 5585961999205058560.00000000\n",
      "Iteration 323, loss = 5581706067450551296.00000000\n",
      "Iteration 324, loss = 5577453377653964800.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 325, loss = 5573203928256420864.00000000\n",
      "Iteration 326, loss = 5568957716228983808.00000000\n",
      "Iteration 327, loss = 5564714739456623616.00000000\n",
      "Iteration 328, loss = 5560474995447911424.00000000\n",
      "Iteration 329, loss = 5556238481787098112.00000000\n",
      "Iteration 330, loss = 5552005195919637504.00000000\n",
      "Iteration 331, loss = 5547775135033309184.00000000\n",
      "Iteration 332, loss = 5543548297572480000.00000000\n",
      "Iteration 333, loss = 5539324680248062976.00000000\n",
      "Iteration 334, loss = 5535104280613127168.00000000\n",
      "Iteration 335, loss = 5530887096680526848.00000000\n",
      "Iteration 336, loss = 5526673126307103744.00000000\n",
      "Iteration 337, loss = 5522462365830150144.00000000\n",
      "Iteration 338, loss = 5518254814003903488.00000000\n",
      "Iteration 339, loss = 5514050467814913024.00000000\n",
      "Iteration 340, loss = 5509849324659610624.00000000\n",
      "Iteration 341, loss = 5505651382651204608.00000000\n",
      "Iteration 342, loss = 5501456639281938432.00000000\n",
      "Iteration 343, loss = 5497265091072625664.00000000\n",
      "Iteration 344, loss = 5493076736863691776.00000000\n",
      "Iteration 345, loss = 5488891573805715456.00000000\n",
      "Iteration 346, loss = 5484709599339632640.00000000\n",
      "Iteration 347, loss = 5480530810684749824.00000000\n",
      "Iteration 348, loss = 5476355206486874112.00000000\n",
      "Iteration 349, loss = 5472182783322056704.00000000\n",
      "Iteration 350, loss = 5468013539796520960.00000000\n",
      "Iteration 351, loss = 5463847471990187008.00000000\n",
      "Iteration 352, loss = 5459684578203070464.00000000\n",
      "Iteration 353, loss = 5455524856757154816.00000000\n",
      "Iteration 354, loss = 5451368304017208320.00000000\n",
      "Iteration 355, loss = 5447214918947221504.00000000\n",
      "Iteration 356, loss = 5443064697947348992.00000000\n",
      "Iteration 357, loss = 5438917638644895744.00000000\n",
      "Iteration 358, loss = 5434773738948235264.00000000\n",
      "Iteration 359, loss = 5430632997081461760.00000000\n",
      "Iteration 360, loss = 5426495409641960448.00000000\n",
      "Iteration 361, loss = 5422360974885474304.00000000\n",
      "Iteration 362, loss = 5418229689854926848.00000000\n",
      "Iteration 363, loss = 5414101552264246272.00000000\n",
      "Iteration 364, loss = 5409976560119965696.00000000\n",
      "Iteration 365, loss = 5405854710739336192.00000000\n",
      "Iteration 366, loss = 5401736002148755456.00000000\n",
      "Iteration 367, loss = 5397620431265684480.00000000\n",
      "Iteration 368, loss = 5393507996122457088.00000000\n",
      "Iteration 369, loss = 5389398694200104960.00000000\n",
      "Iteration 370, loss = 5385292523224337408.00000000\n",
      "Iteration 371, loss = 5381189480902809600.00000000\n",
      "Iteration 372, loss = 5377089564421515264.00000000\n",
      "Iteration 373, loss = 5372992771412002816.00000000\n",
      "Iteration 374, loss = 5368899100057369600.00000000\n",
      "Iteration 375, loss = 5364808547557415936.00000000\n",
      "Iteration 376, loss = 5360721111599031296.00000000\n",
      "Iteration 377, loss = 5356636790168456192.00000000\n",
      "Iteration 378, loss = 5352555580373448704.00000000\n",
      "Iteration 379, loss = 5348477479923940352.00000000\n",
      "Iteration 380, loss = 5344402486824477696.00000000\n",
      "Iteration 381, loss = 5340330597893536768.00000000\n",
      "Iteration 382, loss = 5336261812331512832.00000000\n",
      "Iteration 383, loss = 5332196126246664192.00000000\n",
      "Iteration 384, loss = 5328133537504496640.00000000\n",
      "Iteration 385, loss = 5324074044134187008.00000000\n",
      "Iteration 386, loss = 5320017644117479424.00000000\n",
      "Iteration 387, loss = 5315964334307338240.00000000\n",
      "Iteration 388, loss = 5311914112853609472.00000000\n",
      "Iteration 389, loss = 5307866977207294976.00000000\n",
      "Iteration 390, loss = 5303822925270523904.00000000\n",
      "Iteration 391, loss = 5299781954022583296.00000000\n",
      "Iteration 392, loss = 5295744061566053376.00000000\n",
      "Iteration 393, loss = 5291709245807125504.00000000\n",
      "Iteration 394, loss = 5287677504264137728.00000000\n",
      "Iteration 395, loss = 5283648834311842816.00000000\n",
      "Iteration 396, loss = 5279623233839348736.00000000\n",
      "Iteration 397, loss = 5275600700725173248.00000000\n",
      "Iteration 398, loss = 5271581232274457600.00000000\n",
      "Iteration 399, loss = 5267564826158862336.00000000\n",
      "Iteration 400, loss = 5263551480034222080.00000000\n",
      "Iteration 401, loss = 5259541191867612160.00000000\n",
      "Iteration 402, loss = 5255533958916932608.00000000\n",
      "Iteration 403, loss = 5251529778844180480.00000000\n",
      "Iteration 404, loss = 5247528650014817280.00000000\n",
      "Iteration 405, loss = 5243530569787394048.00000000\n",
      "Iteration 406, loss = 5239535535506186240.00000000\n",
      "Iteration 407, loss = 5235543544359011328.00000000\n",
      "Iteration 408, loss = 5231554595499094016.00000000\n",
      "Iteration 409, loss = 5227568685773101056.00000000\n",
      "Iteration 410, loss = 5223585812732914688.00000000\n",
      "Iteration 411, loss = 5219605974367520768.00000000\n",
      "Iteration 412, loss = 5215629168089985024.00000000\n",
      "Iteration 413, loss = 5211655391405285376.00000000\n",
      "Iteration 414, loss = 5207684642679109632.00000000\n",
      "Iteration 415, loss = 5203716918878737408.00000000\n",
      "Iteration 416, loss = 5199752218484958208.00000000\n",
      "Iteration 417, loss = 5195790538882211840.00000000\n",
      "Iteration 418, loss = 5191831877434723328.00000000\n",
      "Iteration 419, loss = 5187876232541019136.00000000\n",
      "Iteration 420, loss = 5183923601029023744.00000000\n",
      "Iteration 421, loss = 5179973981395386368.00000000\n",
      "Iteration 422, loss = 5176027370486605824.00000000\n",
      "Iteration 423, loss = 5172083766685240320.00000000\n",
      "Iteration 424, loss = 5168143167657806848.00000000\n",
      "Iteration 425, loss = 5164205570867930112.00000000\n",
      "Iteration 426, loss = 5160270973828384768.00000000\n",
      "Iteration 427, loss = 5156339374840471552.00000000\n",
      "Iteration 428, loss = 5152410771042380800.00000000\n",
      "Iteration 429, loss = 5148485160742179840.00000000\n",
      "Iteration 430, loss = 5144562540903662592.00000000\n",
      "Iteration 431, loss = 5140642910400030720.00000000\n",
      "Iteration 432, loss = 5136726265821277184.00000000\n",
      "Iteration 433, loss = 5132812605198831616.00000000\n",
      "Iteration 434, loss = 5128901926914225152.00000000\n",
      "Iteration 435, loss = 5124994228227504128.00000000\n",
      "Iteration 436, loss = 5121089506305115136.00000000\n",
      "Iteration 437, loss = 5117187759333729280.00000000\n",
      "Iteration 438, loss = 5113288985659532288.00000000\n",
      "Iteration 439, loss = 5109393182019247104.00000000\n",
      "Iteration 440, loss = 5105500346543718400.00000000\n",
      "Iteration 441, loss = 5101610477123388416.00000000\n",
      "Iteration 442, loss = 5097723571044334592.00000000\n",
      "Iteration 443, loss = 5093839627005801472.00000000\n",
      "Iteration 444, loss = 5089958641966871552.00000000\n",
      "Iteration 445, loss = 5086080613614799872.00000000\n",
      "Iteration 446, loss = 5082205540332734464.00000000\n",
      "Iteration 447, loss = 5078333419252061184.00000000\n",
      "Iteration 448, loss = 5074464248101796864.00000000\n",
      "Iteration 449, loss = 5070598024579044352.00000000\n",
      "Iteration 450, loss = 5066734747338278912.00000000\n",
      "Iteration 451, loss = 5062874413739318272.00000000\n",
      "Iteration 452, loss = 5059017020730541056.00000000\n",
      "Iteration 453, loss = 5055162566767548416.00000000\n",
      "Iteration 454, loss = 5051311049980301312.00000000\n",
      "Iteration 455, loss = 5047462467493121024.00000000\n",
      "Iteration 456, loss = 5043616816911631360.00000000\n",
      "Iteration 457, loss = 5039774096336838656.00000000\n",
      "Iteration 458, loss = 5035934303411060736.00000000\n",
      "Iteration 459, loss = 5032097436171236352.00000000\n",
      "Iteration 460, loss = 5028263492329334784.00000000\n",
      "Iteration 461, loss = 5024432469587698688.00000000\n",
      "Iteration 462, loss = 5020604365750870016.00000000\n",
      "Iteration 463, loss = 5016779178419020800.00000000\n",
      "Iteration 464, loss = 5012956905230432256.00000000\n",
      "Iteration 465, loss = 5009137544617733120.00000000\n",
      "Iteration 466, loss = 5005321094112678912.00000000\n",
      "Iteration 467, loss = 5001507550867011584.00000000\n",
      "Iteration 468, loss = 4997696913194435584.00000000\n",
      "Iteration 469, loss = 4993889178935591936.00000000\n",
      "Iteration 470, loss = 4990084346009141248.00000000\n",
      "Iteration 471, loss = 4986282412073931776.00000000\n",
      "Iteration 472, loss = 4982483374357645312.00000000\n",
      "Iteration 473, loss = 4978687231708928000.00000000\n",
      "Iteration 474, loss = 4974893980551693312.00000000\n",
      "Iteration 475, loss = 4971103620147572736.00000000\n",
      "Iteration 476, loss = 4967316147264523264.00000000\n",
      "Iteration 477, loss = 4963531560095656960.00000000\n",
      "Iteration 478, loss = 4959749856793253888.00000000\n",
      "Iteration 479, loss = 4955971034163662848.00000000\n",
      "Iteration 480, loss = 4952195090720093184.00000000\n",
      "Iteration 481, loss = 4948422024130429952.00000000\n",
      "Iteration 482, loss = 4944651832522141696.00000000\n",
      "Iteration 483, loss = 4940884513206984704.00000000\n",
      "Iteration 484, loss = 4937120064382586880.00000000\n",
      "Iteration 485, loss = 4933358483115061248.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 486, loss = 4929599768544625664.00000000\n",
      "Iteration 487, loss = 4925843918046291968.00000000\n",
      "Iteration 488, loss = 4922090927959296000.00000000\n",
      "Iteration 489, loss = 4918340798167507968.00000000\n",
      "Iteration 490, loss = 4914593524831245312.00000000\n",
      "Iteration 491, loss = 4910849107355441152.00000000\n",
      "Iteration 492, loss = 4907107542402561024.00000000\n",
      "Iteration 493, loss = 4903368827844619264.00000000\n",
      "Iteration 494, loss = 4899632962461060096.00000000\n",
      "Iteration 495, loss = 4895899942693455872.00000000\n",
      "Iteration 496, loss = 4892169767774169088.00000000\n",
      "Iteration 497, loss = 4888442434381062144.00000000\n",
      "Iteration 498, loss = 4884717940951998464.00000000\n",
      "Iteration 499, loss = 4880996284840058880.00000000\n",
      "Iteration 500, loss = 4877277465128573952.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giaco/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size=64, beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(16, 8, 1), learning_rate='constant',\n",
       "             learning_rate_init=0.01, max_iter=500, momentum=0.9,\n",
       "             n_iter_no_change=1000, nesterovs_momentum=True, power_t=0.5,\n",
       "             random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import sklearn.neural_network\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "lr = 0.01           # learning rate\n",
    "nn = [2, 16, 8, 1]  # n√∫mero de neuronas por capa.\n",
    "\n",
    "# Creamos el objeto del modelo de red neuronal multicapa.\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp = imp.fit(x_train)\n",
    "\n",
    "x_train = imp.transform(x_train)\n",
    "\n",
    "clf = sk.neural_network.MLPRegressor(solver='sgd', \n",
    "                                     learning_rate_init=lr, \n",
    "                                     hidden_layer_sizes=tuple(nn[1:]),\n",
    "                                     verbose=True,\n",
    "                                     n_iter_no_change=1000,\n",
    "                                     batch_size = 64,\n",
    "                                     max_iter = 500)\n",
    "\n",
    "\n",
    "# Y lo entrenamos con nuestro datos.\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1597524.6606868205\n"
     ]
    }
   ],
   "source": [
    "print('MAE:', mean_absolute_error(y_test, prediccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
